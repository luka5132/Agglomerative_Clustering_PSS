{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Train finetuned Image model, code by Ruben van Heusden\n",
        "\n",
        "https://github.com/RubenvanHeusden"
      ],
      "metadata": {
        "id": "3IOqn41XZ4xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import tensorflow\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from metricutils import *\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tensorflow.config.list_physical_devices('GPU')))\n",
        "\n",
        "# https://stackoverflow.com/questions/30811918/saving-dictionary-of-numpy-arrays\n",
        "\n",
        "\n",
        "class ImageModelWiedemann:\n",
        "    def __init__(self, learning_rate=0.00001):\n",
        "\n",
        "        # We use the VGG16 model pretrained on the imagenet corpus\n",
        "        # As the basis of our network.\n",
        "        model_vgg16 = VGG16(weights='imagenet', include_top=False,\n",
        "                            input_shape=(300, 300, 3))\n",
        "\n",
        "        # We don't want to train the first 13 layers of the VGG16 model\n",
        "        # We will add our own tower to this later. It is common in the literature\n",
        "        # To only freeze the first 4 of the 5 convolutional layers so that\n",
        "        # the network can still learn to adjust some of the filters to specifics\n",
        "        # of the dataset\n",
        "        for l in model_vgg16.layers[:13]:\n",
        "            l.trainable = False\n",
        "\n",
        "        top_model = Flatten()(model_vgg16.output)\n",
        "        drop1 = Dropout(0.5)(top_model)\n",
        "        dense1 = Dense(512)(drop1)\n",
        "        relu1 = LeakyReLU()(dense1)\n",
        "        drop2 = Dropout(0.5)(relu1)\n",
        "        dense2 = Dense(256)(drop2)\n",
        "        relu2 = LeakyReLU()(dense2)\n",
        "\n",
        "        # After the output of the model, we pass the output through\n",
        "        # A final linear layer and a sigmoid to obtain values for prediction\n",
        "        model_output = Dense(1, activation=\"sigmoid\")(relu2)\n",
        "\n",
        "        model = Model(model_vgg16.input, model_output)\n",
        "        # Set up the optimzation steps as described in the original\n",
        "        # wiedemann paper.\n",
        "        model.compile(loss='binary_crossentropy', optimizer=Nadam(learning_rate=learning_rate),\n",
        "                      metrics=['AUC'])\n",
        "\n",
        "        self.intermediate_activation = Model(model_vgg16.input, dense1)\n",
        "        self.intermediate_activation.compile()\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def train(self, train_data, num_epochs=20):\n",
        "        self.model.fit(train_data, epochs=num_epochs)\n",
        "\n",
        "    def predict(self, test_data):\n",
        "        y_predict = self.model.predict(test_data, verbose=True)\n",
        "        return y_predict\n",
        "\n",
        "    def store_vectors(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "def prepare_df_for_model(dataframe):\n",
        "    dataframe['png'] = dataframe.name + '-' + dataframe.page.astype(str) + '.png'\n",
        "    dataframe['label'] = dataframe['label'].astype(str)\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "def prepare_test_streams(test_subdataframe, png_folder,\n",
        "                         batch_size):\n",
        "\n",
        "    subtest_generator = ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_input).flow_from_dataframe(\n",
        "        dataframe=test_subdataframe,\n",
        "        directory=png_folder,\n",
        "        x_col='png',\n",
        "        y_col='label',\n",
        "        target_size=(300, 300),\n",
        "        class_mode=None,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        seed=42,\n",
        "        validate_filenames=True,\n",
        "    )\n",
        "\n",
        "    return subtest_generator\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    train_dataframe = prepare_df_for_model(pd.read_csv(args.train_dataframe))\n",
        "    test_dataframe = prepare_df_for_model(pd.read_csv(args.test_dataframe))\n",
        "\n",
        "    train_gen = ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_input).flow_from_dataframe(\n",
        "        dataframe=train_dataframe,\n",
        "        directory=args.train_png_folder,\n",
        "        x_col='png',\n",
        "        y_col='label',\n",
        "        target_size=(300, 300),\n",
        "        class_mode='binary',\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "        validate_filenames=True)\n",
        "\n",
        "    # We either want to train our own model and save it, or use a\n",
        "    # Model we trained ourselves, and only run the prediction step.\n",
        "\n",
        "    model = ImageModelWiedemann(learning_rate=args.learning_rate)\n",
        "    if args.from_trained:\n",
        "        model.model = load_model(args.save_path)\n",
        "    else:\n",
        "        model.train(train_data=train_gen, num_epochs=args.num_epochs)\n",
        "        model.model.save(args.save_path)\n",
        "\n",
        "    stream_predictions = {}\n",
        "    vector_outputs = {}\n",
        "    raw_outputs = {}\n",
        "\n",
        "    for doc_id, stream in test_dataframe.groupby('name'):\n",
        "        stream['page'] = stream['page'].astype(int)\n",
        "        sorted_stream = stream.sort_values(by='page')\n",
        "\n",
        "        test_data = prepare_test_streams(sorted_stream, args.test_png_folder,\n",
        "                                         args.batch_size)\n",
        "\n",
        "        out = model.predict(test_data).squeeze()\n",
        "        stream_prediction = np.round(out).astype(int).tolist()\n",
        "        stream_predictions[doc_id] = stream_prediction\n",
        "        raw_outputs[doc_id] = out.tolist()\n",
        "\n",
        "        vectors = model.intermediate_activation.predict(test_data)\n",
        "        vector_outputs[doc_id] = vectors\n",
        "\n",
        "    test_dataframe['label'] = test_dataframe['label'].astype(int)\n",
        "\n",
        "    corpus_train = args.train_dataframe.split('/')[-3]\n",
        "    corpus_test = args.test_dataframe.split('/')[-3]\n",
        "    gold_standard = get_ground_truth_from_dataframe(test_dataframe,\n",
        "                                                    'label')\n",
        "\n",
        "    save_name = \"text_trained_on_%s_tested_on_%s_IMAGE_CNN\" % (corpus_train, corpus_test)\n",
        "    np.save(os.path.join(save_name, 'vectors.npy'), vector_outputs)\n",
        "    if not os.path.exists(save_name):\n",
        "        # Create a new directory because it does not exist\n",
        "        os.makedirs(save_name)\n",
        "\n",
        "    with open(os.path.join(save_name, 'predictions.json'), 'w') as f:\n",
        "        json.dump(stream_predictions, f)\n",
        "\n",
        "    with open(os.path.join(save_name, 'gold_standard.json'), 'w') as f:\n",
        "        json.dump(gold_standard, f)\n",
        "\n",
        "    with open(os.path.join(save_name, 'raw_scores.json'), 'w') as f:\n",
        "        json.dump(raw_outputs, f)\n",
        "\n",
        "    evaluation_report(gold_standard, stream_predictions)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--train_dataframe', type=str, required=True)\n",
        "    parser.add_argument('--test_dataframe', type=str, required=True)\n",
        "    parser.add_argument('--train_png_folder', type=str, required=True)\n",
        "    parser.add_argument('--test_png_folder', type=str, required=True)\n",
        "    parser.add_argument('--learning_rate',type=float, default=0.00001)\n",
        "    parser.add_argument('--batch_size', type=int, default=256)\n",
        "    parser.add_argument('--num_epochs', type=int, default=20)\n",
        "    parser.add_argument('--save_path', type=str)\n",
        "    parser.add_argument('--from_trained', type=bool, default=False)\n",
        "\n",
        "    arguments = parser.parse_args()\n",
        "    main(arguments)"
      ],
      "metadata": {
        "id": "FEtKIS5TaO_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train finetuned Text model, code by Ruben van Heusden"
      ],
      "metadata": {
        "id": "WfbSwN_TaXnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This file contains a PyTorch style dataloader that can be used to load in the\n",
        "data from different WOB data sources, for easier use with the different models\n",
        "that are used in this project.\n",
        "\n",
        "Because we have two modalities for each page (both visual and textual) we\n",
        "construct the dataset in such a way that we can use one dataloader for\n",
        "the whole dataset, and just select what information we want to use based on\n",
        "which model we are training.\n",
        "\n",
        "For some of the models that we are using we need to specify a specific\n",
        "pipeline for the preprocessing of the image, because we need to do some\n",
        "resampling / rescaling, such as for the VGG16 model.\n",
        "\n",
        "Although the exact structure of the dataset does not have to specified\n",
        "in advance (train, val and test split) we will require this for the first versions\n",
        "of the algorithms, just so that we know for sure that we actually do the\n",
        "proper things for the first two corpora. Later we can then make a more\n",
        "complicated version that will allow us a bit more freedom in how we interact\n",
        "with the datasets.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "# Local imports\n",
        "import metricutils\n",
        "\n",
        "\n",
        "def load_text_dataframe(dataframe_path: str, nan_fill_value: str = ''):\n",
        "    \"\"\"\n",
        "    In this method we load in the csv text OCR dataset.\n",
        "    We do this in a separate method because the dataset requires\n",
        "    some preprocessing to make sure that the data is loading in properly\n",
        "    and that we can actually combine it with the images that we get from\n",
        "    the image loading module.\n",
        "\n",
        "    :param nan_fill_value: string specifying what value to use if a value\n",
        "    is missing in the text entry of a page.\n",
        "    :param dataframe_path: string specifying the path to the dataframe\n",
        "    that contains the text of the pages and the gold standard.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    ocr_dataframe = pd.read_csv(dataframe_path)\n",
        "\n",
        "    # As in principle these pages could be unordered, we want to make sure\n",
        "    # stream is ordered in ascending order by the page number so that it\n",
        "    # lines up with the gold standard data.\n",
        "\n",
        "    ocr_dataframe['page'] = ocr_dataframe['page'].astype(int)\n",
        "    # sorting by name because this is just the name of the stream and this\n",
        "    # way we can sort each stream on page number properly.\n",
        "    ocr_dataframe = ocr_dataframe.sort_values(by=['name', 'page'])\n",
        "\n",
        "    ocr_dataframe.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    # Fill any nan values in the text with the value specified\n",
        "    # in 'nan_fill_value'\n",
        "    ocr_dataframe.text.fillna(nan_fill_value, inplace=True)\n",
        "\n",
        "    return ocr_dataframe\n"
      ],
      "metadata": {
        "id": "6bbg55Ntae1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "For now, remove any of the references to the multi class options,\n",
        "as we will not really use this for our experiments.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import re, math\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "print(np.__version__)\n",
        "from gensim.models.fasttext import load_facebook_model\n",
        "\n",
        "ft = load_facebook_model(\n",
        "    \"/ivi/ilps/personal/rheusde/WOOIR/models/cc.nl.300.bin\")\n",
        "print('---Fasttext model has been loaded---')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.utils import *\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "from dataloading import *\n",
        "from metricutils import *\n",
        "\n",
        "\n",
        "def get_data_instances(df):\n",
        "    data_instances = []\n",
        "    for index, row in df.iterrows():\n",
        "        data_instances.append([row['label'], row['text']])\n",
        "    return data_instances\n",
        "\n",
        "\n",
        "def simple_tokenizer(textline: str):\n",
        "    textline = re.sub(r'http\\S+', 'URL', textline)\n",
        "    words = re.compile(r'[#\\w-]+|[^#\\w-]+', re.UNICODE).findall(\n",
        "        textline.strip())\n",
        "    words = [w.strip() for w in words if w.strip() != '']\n",
        "    return words\n",
        "\n",
        "\n",
        "class TextFeatureGenerator(Sequence):\n",
        "    def __init__(self, text_data, batch_size=32):\n",
        "        self.text_data = text_data\n",
        "        self.indices = np.arange(len(self.text_data))\n",
        "        self.batch_size = batch_size\n",
        "        self.sequence_length = 150\n",
        "        self.embedding_dims = 300\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.text_data) / self.batch_size)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_x, batch_y = self.process_text_data(inds)\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def process_text_data(self, inds):\n",
        "\n",
        "        word_embeddings = []\n",
        "        output_labels = []\n",
        "\n",
        "        for index in inds:\n",
        "            word_embeddings.append(\n",
        "                self.text_to_embedding(self.text_data[index][1]))\n",
        "            output_labels.append(self.text_data[index][0])\n",
        "\n",
        "        return np.array(word_embeddings), np.array(output_labels)\n",
        "\n",
        "    def text_to_embedding(self, textsequence):\n",
        "        temp_word = []\n",
        "\n",
        "        # tokenize\n",
        "        sentence = simple_tokenizer(textsequence)\n",
        "\n",
        "        # trim to max sequence length\n",
        "        if len(sentence) > self.sequence_length:\n",
        "            half_idx = int(self.sequence_length / 2)\n",
        "            tmp_sentence = sentence[:half_idx]\n",
        "            tmp_sentence.extend(sentence[(len(sentence) - half_idx):])\n",
        "            sentence = tmp_sentence\n",
        "\n",
        "        # padding\n",
        "        words_to_pad = self.sequence_length - len(sentence)\n",
        "\n",
        "        for i in range(words_to_pad):\n",
        "            sentence.append('PADDING_TOKEN')\n",
        "\n",
        "        # create data input for words\n",
        "        for w_i, word in enumerate(sentence):\n",
        "\n",
        "            if word == 'PADDING_TOKEN':\n",
        "                word_vector = [0] * self.embedding_dims\n",
        "            else:\n",
        "                word_vector = ft.wv[word.lower()]\n",
        "\n",
        "            temp_word.append(word_vector)\n",
        "\n",
        "        return temp_word\n",
        "\n",
        "\n",
        "class TextModelWiedemann:\n",
        "    def __init__(self, nb_embedding_dims=300, nb_sequence_length=150):\n",
        "\n",
        "        filter_sizes = (3, 4, 5)\n",
        "\n",
        "        model_input_tp = Input(shape=(nb_sequence_length, nb_embedding_dims))\n",
        "        gru_block_tp = Bidirectional(\n",
        "            GRU(128, dropout=0.5, return_sequences=True))(\n",
        "            model_input_tp)\n",
        "        conv_blocks_tp = []\n",
        "        for sz in filter_sizes:\n",
        "            conv = Conv1D(\n",
        "                filters=200,\n",
        "                kernel_size=sz,\n",
        "                padding=\"same\",\n",
        "                strides=1\n",
        "            )(gru_block_tp)\n",
        "            conv = LeakyReLU()(conv)\n",
        "            conv = GlobalMaxPooling1D()(conv)\n",
        "            conv = Dropout(0.5)(conv)\n",
        "            conv_blocks_tp.append(conv)\n",
        "        model_concatenated_tp = concatenate(conv_blocks_tp)\n",
        "        model_concatenated_tp = Dense(128)(model_concatenated_tp)\n",
        "        model_concatenated_tp = LeakyReLU()(model_concatenated_tp)\n",
        "\n",
        "        model_output = Dense(1, activation=\"sigmoid\")(model_concatenated_tp)\n",
        "\n",
        "        # combine final model\n",
        "        model = Model(model_input_tp, model_output)\n",
        "        model.compile(loss='binary_crossentropy', optimizer='nadam',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def train(self, train_data, batch_size, num_epochs):\n",
        "        # Here we write a very simple training loop\n",
        "        self.model.fit(TextFeatureGenerator(train_data, batch_size=batch_size),\n",
        "                       epochs=num_epochs)\n",
        "\n",
        "    def predict(self, test_dataframe, batch_size):\n",
        "        all_stream_predictions = {}\n",
        "        raw_predictions = {}\n",
        "\n",
        "        for name, sub_df in tqdm(test_dataframe.groupby(\"name\")):\n",
        "            predictions = self.model.predict(\n",
        "                TextFeatureGenerator(get_data_instances(sub_df),\n",
        "                                     batch_size=batch_size)).squeeze()\n",
        "\n",
        "            final_predictions = predictions.round().astype(int).tolist()\n",
        "            all_stream_predictions[name] = final_predictions\n",
        "            raw_predictions[name] = predictions.tolist()\n",
        "\n",
        "            if sub_df.shape[0] == 1:\n",
        "                all_stream_predictions[name] = [predictions]\n",
        "                raw_predictions[name] = [final_predictions]                \n",
        "\n",
        "        return all_stream_predictions, raw_predictions\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # Our first step here is to set up the model from its class\n",
        "    if args.do_train:\n",
        "        text_model = TextModelWiedemann()\n",
        "    else:\n",
        "        text_model = TextModelWiedemann()\n",
        "        text_model.model = load_model(args.save_path)\n",
        "    text_model.ft = ft\n",
        "    print('Model has been loaded')\n",
        "\n",
        "    # Now we also have to load the training and test datasets\n",
        "    train_dataframe = load_text_dataframe(args.train_dataframe)\n",
        "    test_dataframe = load_text_dataframe(args.test_dataframe)\n",
        "    print('data has been loaded')\n",
        "    gold_standard_dict = get_ground_truth_from_dataframe(test_dataframe,\n",
        "                                                         col='label')\n",
        "\n",
        "    \n",
        "    for key, val in gold_standard_dict.items():\n",
        "        val[0] = 1\n",
        "        gold_standard_dict[key] = val\n",
        "\n",
        "    train_instances = get_data_instances(train_dataframe)\n",
        "    print('training about to start')\n",
        "    if args.do_train:\n",
        "        text_model.train(train_instances, batch_size=args.batch_size,\n",
        "                         num_epochs=args.num_epochs)\n",
        "        text_model.model.save(args.save_path)\n",
        "    print('training has finised')\n",
        "    prediction_dict, raw_dict = text_model.predict(test_dataframe,\n",
        "                                                   batch_size=args.batch_size)\n",
        "\n",
        "    corpus_train = args.train_dataframe.split('/')[-3]\n",
        "    corpus_test = args.test_dataframe.split('/')[-3]\n",
        "\n",
        "    save_name = \"text_trained_on_%s_tested_on_%s\" % (corpus_train, corpus_test)\n",
        "\n",
        "    if not os.path.exists(save_name):\n",
        "        # Create a new directory because it does not exist\n",
        "        os.makedirs(save_name)\n",
        "\n",
        "    with open(os.path.join(save_name, 'predictions.json'), 'w') as f:\n",
        "        json.dump(prediction_dict, f)\n",
        "\n",
        "    with open(os.path.join(save_name, 'gold_standard.json'), 'w') as f:\n",
        "        json.dump(gold_standard_dict, f)\n",
        "\n",
        "    with open(os.path.join(save_name, 'raw_scores.json'), 'w') as f:\n",
        "        json.dump(raw_dict, f)\n",
        "\n",
        "    evaluation_report(gold_standard_dict, prediction_dict)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--train_dataframe', type=str, required=True)\n",
        "    parser.add_argument('--test_dataframe', type=str, required=True)\n",
        "    parser.add_argument('--num_epochs', type=int, default=20)\n",
        "    parser.add_argument('--save_path', type=str)\n",
        "    parser.add_argument('--batch_size', type=int, default=256)\n",
        "    parser.add_argument('--do_train', type=bool, default=False)\n",
        "\n",
        "    arguments = parser.parse_args()\n",
        "    main(arguments)"
      ],
      "metadata": {
        "id": "-RD93wB-anoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting finetuned Vector representations of CNN"
      ],
      "metadata": {
        "id": "SjjSEZyjas2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pss_model import compile_model_singlepage,ValidationCheckpoint,ImageFeatureGenerator\n",
        "from tqdm import tqdm\n",
        "from PIL import Image \n",
        "import os\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "import argparse \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def prepare_df_for_model(dataframe):\n",
        "    dataframe['png'] = dataframe.name + '-' + dataframe.page.astype(str) + '.png'\n",
        "    dataframe['label'] = dataframe['label'].astype(str)\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "def prepare_test_streams(test_subdataframe, png_folder,\n",
        "                         batch_size):\n",
        "\n",
        "    subtest_generator = ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_input).flow_from_dataframe(\n",
        "        dataframe=test_subdataframe,\n",
        "        directory=png_folder,\n",
        "        x_col='png',\n",
        "        y_col='label',\n",
        "        target_size=(224, 224),\n",
        "        class_mode=None,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        seed=42,\n",
        "        validate_filenames=True,\n",
        "    )\n",
        "\n",
        "    return subtest_generator\n",
        "\n",
        "def main():\n",
        "  Image.MAX_IMAGE_PIXELS = 1000000000\n",
        "\n",
        "\n",
        "  model = load_model(path_to_model)\n",
        "  layer_name = 'dense'\n",
        "  to_vector_model= Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "  \n",
        "  train_data = prepare_df_for_model(pd.read_csv('test_data.csv'))\n",
        "\n",
        "  for doc_id, stream in tqdm(train_data.groupby('name')):\n",
        "      stream['page'] = stream['page'].astype(int)\n",
        "      sorted_stream = stream.sort_values(by='page')\n",
        "  \n",
        "      train_data = prepare_test_streams(sorted_stream, 'test',\n",
        "                                       256)\n",
        "      vectors = to_vector_model.predict(train_data)\n",
        "  \n",
        "      full_path = '{}/{}.npy'.format('out/ft_test',doc_id)\n",
        "      np.save(full_path, vectors)\n",
        "      \n",
        "main()"
      ],
      "metadata": {
        "id": "bu4Po_wfa0FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting pretrained Vector representations of VGG16"
      ],
      "metadata": {
        "id": "Hl1ovFKoZknQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTo1ZWBZZOjb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pss_model import compile_model_singlepage,ValidationCheckpoint,ImageFeatureGenerator\n",
        "from tqdm import tqdm\n",
        "from PIL import Image \n",
        "import os\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "import argparse \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def prepare_df_for_model(dataframe):\n",
        "    dataframe['png'] = dataframe.name + '-' + dataframe.page.astype(str) + '.png'\n",
        "    dataframe['label'] = dataframe['label'].astype(str)\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "def prepare_test_streams(test_subdataframe, png_folder,\n",
        "                         batch_size):\n",
        "\n",
        "    subtest_generator = ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_input).flow_from_dataframe(\n",
        "        dataframe=test_subdataframe,\n",
        "        directory=png_folder,\n",
        "        x_col='png',\n",
        "        y_col='label',\n",
        "        target_size=(224, 224),\n",
        "        class_mode=None,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        seed=42,\n",
        "        validate_filenames=True,\n",
        "    )\n",
        "\n",
        "    return subtest_generator\n",
        "\n",
        "def main():\n",
        "  Image.MAX_IMAGE_PIXELS = 1000000000\n",
        "\n",
        "\n",
        "  model_vgg16 = VGG16(weights = 'imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "  layer_name = 'fc2'\n",
        "  to_vector_model= Model(inputs=model_vgg16.input, outputs=model_vgg16.get_layer(layer_name).output)\n",
        "  \n",
        "  train_data = prepare_df_for_model(pd.read_csv('test_data.csv'))\n",
        "\n",
        "  for doc_id, stream in tqdm(train_data.groupby('name')):\n",
        "      stream['page'] = stream['page'].astype(int)\n",
        "      sorted_stream = stream.sort_values(by='page')\n",
        "  \n",
        "      train_data = prepare_test_streams(sorted_stream, 'test',\n",
        "                                       256)\n",
        "      vectors = to_vector_model.predict(train_data)\n",
        "  \n",
        "      full_path = '{}/{}.npy'.format('out/vgg_test',doc_id)\n",
        "      np.save(full_path, vectors)\n",
        "      \n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get finetunted text vectors"
      ],
      "metadata": {
        "id": "N8BDYwHXZ1XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import re, math\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "print(np.__version__)\n",
        "from gensim.models.fasttext import load_facebook_model\n",
        "\n",
        "ft = load_facebook_model(\n",
        "    \"cc.nl.300.bin\")\n",
        "print('---Fasttext model has been loaded---')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.utils import *\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "from dataloading import *\n",
        "from metricutils import *\n",
        "\n",
        "def get_data_instances(df):\n",
        "    data_instances = []\n",
        "    for index, row in df.iterrows():\n",
        "        data_instances.append([row['label'], row['text']])\n",
        "    return data_instances\n",
        "\n",
        "\n",
        "def simple_tokenizer(textline: str):\n",
        "    textline = re.sub(r'http\\S+', 'URL', textline)\n",
        "    words = re.compile(r'[#\\w-]+|[^#\\w-]+', re.UNICODE).findall(\n",
        "        textline.strip())\n",
        "    words = [w.strip() for w in words if w.strip() != '']\n",
        "    return words\n",
        "\n",
        "\n",
        "class TextFeatureGenerator(Sequence):\n",
        "    def __init__(self, text_data, batch_size=32):\n",
        "        self.text_data = text_data\n",
        "        self.indices = np.arange(len(self.text_data))\n",
        "        self.batch_size = batch_size\n",
        "        self.sequence_length = 150\n",
        "        self.embedding_dims = 300\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.text_data) / self.batch_size)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_x, batch_y = self.process_text_data(inds)\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def process_text_data(self, inds):\n",
        "\n",
        "        word_embeddings = []\n",
        "        output_labels = []\n",
        "\n",
        "        for index in inds:\n",
        "            word_embeddings.append(\n",
        "                self.text_to_embedding(self.text_data[index][1]))\n",
        "            output_labels.append(self.text_data[index][0])\n",
        "\n",
        "        return np.array(word_embeddings), np.array(output_labels)\n",
        "\n",
        "    def text_to_embedding(self, textsequence):\n",
        "        temp_word = []\n",
        "\n",
        "        # tokenize\n",
        "        sentence = simple_tokenizer(textsequence)\n",
        "\n",
        "        # trim to max sequence length\n",
        "        if len(sentence) > self.sequence_length:\n",
        "            half_idx = int(self.sequence_length / 2)\n",
        "            tmp_sentence = sentence[:half_idx]\n",
        "            tmp_sentence.extend(sentence[(len(sentence) - half_idx):])\n",
        "            sentence = tmp_sentence\n",
        "\n",
        "        # padding\n",
        "        words_to_pad = self.sequence_length - len(sentence)\n",
        "\n",
        "        for i in range(words_to_pad):\n",
        "            sentence.append('PADDING_TOKEN')\n",
        "\n",
        "        # create data input for words\n",
        "        for w_i, word in enumerate(sentence):\n",
        "\n",
        "            if word == 'PADDING_TOKEN':\n",
        "                word_vector = [0] * self.embedding_dims\n",
        "            else:\n",
        "                word_vector = ft.wv[word.lower()]\n",
        "\n",
        "            temp_word.append(word_vector)\n",
        "\n",
        "        return temp_word\n",
        "\n",
        "\n",
        "class TextModelWiedemann:\n",
        "    def __init__(self, nb_embedding_dims=300, nb_sequence_length=150):\n",
        "\n",
        "        filter_sizes = (3, 4, 5)\n",
        "\n",
        "        model_input_tp = Input(shape=(nb_sequence_length, nb_embedding_dims))\n",
        "        gru_block_tp = Bidirectional(\n",
        "            GRU(128, dropout=0.5, return_sequences=True))(\n",
        "            model_input_tp)\n",
        "        conv_blocks_tp = []\n",
        "        for sz in filter_sizes:\n",
        "            conv = Conv1D(\n",
        "                filters=200,\n",
        "                kernel_size=sz,\n",
        "                padding=\"same\",\n",
        "                strides=1\n",
        "            )(gru_block_tp)\n",
        "            conv = LeakyReLU()(conv)\n",
        "            conv = GlobalMaxPooling1D()(conv)\n",
        "            conv = Dropout(0.5)(conv)\n",
        "            conv_blocks_tp.append(conv)\n",
        "        model_concatenated_tp = concatenate(conv_blocks_tp)\n",
        "        model_concatenated_tp = Dense(128)(model_concatenated_tp)\n",
        "        model_concatenated_tp = LeakyReLU()(model_concatenated_tp)\n",
        "\n",
        "        model_output = Dense(1, activation=\"sigmoid\")(model_concatenated_tp)\n",
        "\n",
        "        # combine final model\n",
        "        model = Model(model_input_tp, model_output)\n",
        "        model.compile(loss='binary_crossentropy', optimizer='nadam',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def train(self, train_data, batch_size, num_epochs):\n",
        "        # Here we write a very simple training loop\n",
        "        self.model.fit(TextFeatureGenerator(train_data, batch_size=batch_size),\n",
        "                       epochs=num_epochs)\n",
        "\n",
        "    def predict(self, test_dataframe, batch_size, save =True):\n",
        "        all_stream_predictions = {}\n",
        "\n",
        "        for name, sub_df in tqdm(test_dataframe.groupby(\"name\")):\n",
        "            predictions = self.model.predict(\n",
        "                TextFeatureGenerator(get_data_instances(sub_df),\n",
        "                                     batch_size=batch_size))\n",
        "            \n",
        "            if save:\n",
        "                np.save('out/text_predictions/{}.npy'.format(name), np.array(predictions))\n",
        "            \n",
        "\n",
        "def main(save_path, test_df, to_vec = True):\n",
        "    # Our first step here is to set up the model from its class\n",
        "    text_model = TextModelWiedemann()\n",
        "    model = load_model(save_path)\n",
        "    \n",
        "    if to_vec:\n",
        "        layer_name = 'dense'\n",
        "        to_vector_model= Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "        text_model.model = to_vector_model\n",
        "    else:\n",
        "        text_model.model = model\n",
        "    text_model.ft = ft\n",
        "    print('Model has been loaded')\n",
        "\n",
        "    # Now we also have to load the training and test datasets\n",
        "    test_dataframe = load_text_dataframe(test_df)\n",
        "    print('data has been loaded')\n",
        "\n",
        "\n",
        "    text_model.predict(test_dataframe,batch_size=256)\n",
        "\n",
        "        \n",
        "sp = 'text_cnn_model'\n",
        "td = 'data_fixed.csv'\n",
        "main(sp,td,False)"
      ],
      "metadata": {
        "id": "P0UvOrg-ZxSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}